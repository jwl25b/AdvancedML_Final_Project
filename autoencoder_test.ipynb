{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27274e07-00d6-40c7-b3ed-fb6ab52bb1e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2869aa-7c43-4887-9602-9669298652fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from permutedMNIST import PermutedMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e90edf-9272-4487-939d-4ed54cd82f85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dims, code_dims):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.code_dims = code_dims\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dims, code_dims),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(code_dims, input_dims),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dims)\n",
    "        encoded_x = self.encoder(x)\n",
    "        reconstructed_x = self.decoder(encoded_x)\n",
    "        return reconstructed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62a8451c-5640-4d1a-a0e6-b8a89c957a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Autoencoder():\n",
    "    def __init__(self, input_dims, code_dims, lr=0.001):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.net = Net(input_dims, code_dims).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr)\n",
    "        \n",
    "        #self.criterion = torch.nn.BCELoss()\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.unreduced_criterion = torch.nn.MSELoss(reduction = 'none')\n",
    "        \n",
    "        self.input_dims = input_dims\n",
    "        self.size = 0\n",
    "        self.mean = 0\n",
    "        self.var = 0\n",
    "        self.std = 0\n",
    "        \n",
    "    def optimize_params(self, x, label):\n",
    "        x = x.to(self.device).reshape(-1, self.input_dims)\n",
    "        label = label.to(self.device).reshape(-1, self.input_dims)\n",
    "        y = self._forward(x)\n",
    "        \n",
    "        self._update_params(y, label)\n",
    "        self.update_statistics(y, label)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def _backward(self, y, label):\n",
    "        self.loss = self.criterion(y, label)\n",
    "        self.loss.backward()\n",
    "        #self.accelerator.backward(self.loss)\n",
    "\n",
    "    def _update_params(self, y, label):\n",
    "        self.optimizer.zero_grad()\n",
    "        self._backward(y, label)\n",
    "        self.optimizer.step()\n",
    "        #self.scheduler.step()  # scheduler step in each iteration\n",
    "        \n",
    "    def update_statistics(self, y, label):\n",
    "        #https://math.stackexchange.com/questions/3604607/can-i-work-out-the-variance-in-batches\n",
    "        with torch.no_grad():\n",
    "            #calculate batchwise loss\n",
    "            new_element_loss = self.unreduced_criterion(y, label)\n",
    "            new_loss = torch.mean(new_element_loss, axis=1)\n",
    "            \n",
    "            #calculate mean, var and batch size\n",
    "            new_mean = torch.mean(new_loss).item()\n",
    "            new_var = torch.var(new_loss).item()\n",
    "            new_size = y.shape[0]\n",
    "            #print(new_mean, new_var, new_size)\n",
    "            \n",
    "            #updating variance\n",
    "            part1 = ((self.size - 1)*self.var + (new_size - 1)*new_var)/(self.size + new_size - 1)\n",
    "            part2 = ((self.size*new_size)*np.square((self.mean - new_mean)))/((self.size + new_size)*(self.size+new_size - 1))\n",
    "            self.var = part1 + part2\n",
    "            \n",
    "            #updating std\n",
    "            self.std = np.sqrt(self.var)\n",
    "            #print(self.std)\n",
    "            \n",
    "            #updating mean\n",
    "            new_sum = torch.sum(new_loss).item()\n",
    "            self.mean = (self.size*self.mean + new_sum)/(self.size + new_size)\n",
    "            \n",
    "            #updating size\n",
    "            self.size += new_size\n",
    "            \n",
    "    def get_reduced_loss(self, y, label):\n",
    "        with torch.no_grad():\n",
    "            y = y.to(self.device).reshape(-1, self.input_dims)\n",
    "            label = label.to(self.device).reshape(-1, self.input_dims)\n",
    "            return self.criterion(y, label).item()\n",
    "    \n",
    "    def get_unreduced_loss(self, y, label):\n",
    "        with torch.no_grad():\n",
    "            y = y.to(self.device).reshape(-1, self.input_dims)\n",
    "            label = label.to(self.device).reshape(-1, self.input_dims)\n",
    "            return self.unreduced_criterion(y, label)\n",
    "    \n",
    "    def get_prediction(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self._forward(x.to(self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7ef606b-24ef-4cca-88a9-4cea7d47034e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MNIST_trainingdata = torchvision.datasets.MNIST('./MNIST/',train=True,download=True,transform=torchvision.transforms.ToTensor())\n",
    "#MNIST_testdata = torchvision.datasets.MNIST('./MNIST/',train=False,download=True,transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "#setup dataloader\n",
    "#MNIST_train_loader = torch.utils.data.DataLoader(MNIST_trainingdata,batch_size=64,shuffle=True)\n",
    "#MNIST_test_loader = torch.utils.data.DataLoader(MNIST_testdata,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c25a214-410f-4bcf-a7d8-14f55ba781b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CIFAR_transforms = transforms.Compose([transforms.Resize((28, 28)),transforms.Grayscale(),transforms.ToTensor()])\n",
    "\n",
    "#CIFAR_trainingdata = torchvision.datasets.CIFAR10('./CIFAR/',train=True,download=True,transform=CIFAR_transforms)\n",
    "#CIFAR_testdata = torchvision.datasets.CIFAR10('./CIFAR/',train=False,download=True,transform=CIFAR_transforms)\n",
    "\n",
    "#setup dataloader\n",
    "#CIFAR_train_loader = torch.utils.data.DataLoader(CIFAR_trainingdata,batch_size=64,shuffle=True)\n",
    "#CIFAR_test_loader = torch.utils.data.DataLoader(CIFAR_testdata,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "966cc02a-f169-4b0d-9a59-136176e46e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_autoencoder(train_loader, test_loader, model):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(5): # We'll train for 5 \"epochs\"\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "\n",
    "        # Evaluation process\n",
    "        for i, data in enumerate(test_loader):\n",
    "            images, _, _ = data # Unpack the data into the images and labels\n",
    "            predicted_output = model.get_prediction(images) # Apply our network to the images\n",
    "            test_loss += model.get_reduced_loss(predicted_output, images) # Add the fit to the loss for tracking purposes\n",
    "\n",
    "        # Training process\n",
    "        for i, data in enumerate(train_loader):\n",
    "            images, _, _ = data # Unpack the data into the images and labels\n",
    "            model.optimize_params(images, images)\n",
    "            train_loss += model.loss.item()\n",
    "            \n",
    "        # Add the current losses to our tracking lists\n",
    "        train_losses += [train_loss/len(train_loader)]\n",
    "        test_losses += [test_loss/len(test_loader)]\n",
    "\n",
    "        # Print the current loss\n",
    "        print(f'Epoch {epoch}, Train loss {train_loss}, Test loss {test_loss}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd9fd711-f7f9-49a4-bb65-1c90e0be7a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MNIST_model = Autoencoder(input_dims=28*28, code_dims=500)\n",
    "#CIFAR_model = Autoencoder(input_dims=28*28, code_dims=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82a73623-26ee-4d68-824d-460446449097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MNIST_model = train_autoencoder(MNIST_train_loader, MNIST_test_loader, MNIST_model)\n",
    "#CIFAR_model = train_autoencoder(CIFAR_train_loader, CIFAR_test_loader, CIFAR_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "216f8e0e-c194-4756-92a3-f5a1f8f2da62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visulize_test_autoencoder(test_loader, models):\n",
    "    images, _ = next(iter(test_loader))\n",
    "    image = images[np.random.randint(0, len(images))]\n",
    "\n",
    "    show_image = image.cpu().detach().numpy().reshape((28,28))\n",
    "    plt.imshow(show_image) # Plot the 28x28 image\n",
    "    plt.show()\n",
    "    \n",
    "    for _, model in models.items():\n",
    "        model_output = model.get_prediction(image)\n",
    "        show_output = model_output[0].cpu().detach().numpy().reshape((28,28))\n",
    "        #plt.imshow(show_output) # Plot the 28x28 image\n",
    "        #plt.show()\n",
    "        \n",
    "        print(\"MNIST loss:\", model.get_reduced_loss(model_output, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d5644c-32a5-4b62-bc4c-e92b0363c32e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#visulize_test_autoencoder(MNIST_test_loader, [MNIST_model, CIFAR_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "483361c0-e39a-4dd8-87fc-dfc67929f0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#visulize_test_autoencoder(CIFAR_test_loader, [MNIST_model, CIFAR_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a572b03-e58d-41c0-a2d3-15f17e66a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_permute_mnist(num_task, batch_size, random_seed):\n",
    "    train_loader = {}\n",
    "    test_loader = {}\n",
    "    np.random.seed(random_seed)\n",
    "    idx = list(range(28 * 28))\n",
    "    for i in range(num_task):\n",
    "        train_loader[i] = torch.utils.data.DataLoader(PermutedMNIST(train=True, permute_idx=idx, task_num = i),\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      num_workers=4)\n",
    "        test_loader[i] = torch.utils.data.DataLoader(PermutedMNIST(train=False, permute_idx=idx, task_num = i),\n",
    "                                                     batch_size=batch_size)\n",
    "        np.random.shuffle(idx)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccceaa3e-f1c8-4abe-8e76-81c2e49ee2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TASK=20\n",
    "BATCH_SIZE=600\n",
    "RANDOM_SEED=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "099521a8-8861-4261-a999-41ec53d87e25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "C:\\Program Files\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:80: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "permute_train_loaders, permute_test_loaders = get_permute_mnist(NUM_TASK, BATCH_SIZE, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c68f7879-3c85-4e01-8371-2b94e2c8df5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels, index = next(iter(permute_train_loaders[1]))\n",
    "image = images[0].cpu().detach().numpy().reshape((28,28))\n",
    "plt.imshow(image) # Plot the 28x28 image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46b6cb42-f057-4668-99fd-f715cbf8239a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss 6.924171455204487, Test loss 3.943579003214836\n",
      "Epoch 1, Train loss 3.239029537886381, Test loss 0.6835575178265572\n",
      "Epoch 2, Train loss 2.207655917853117, Test loss 0.43445930629968643\n",
      "Epoch 3, Train loss 1.6396753462031484, Test loss 0.3139985781162977\n",
      "Epoch 4, Train loss 1.2672805869951844, Test loss 0.2438466539606452\n",
      "Epoch 0, Train loss 6.858460795134306, Test loss 3.93233759701252\n",
      "Epoch 1, Train loss 3.2275188229978085, Test loss 0.6714719831943512\n",
      "Epoch 2, Train loss 2.2341847997158766, Test loss 0.4369999635964632\n",
      "Epoch 3, Train loss 1.6688537104055285, Test loss 0.31980495899915695\n",
      "Epoch 4, Train loss 1.2938870340585709, Test loss 0.2472309973090887\n",
      "Epoch 0, Train loss 6.86918618157506, Test loss 3.9433748722076416\n",
      "Epoch 1, Train loss 3.2440181393176317, Test loss 0.6766751557588577\n",
      "Epoch 2, Train loss 2.1801779698580503, Test loss 0.43380219116806984\n",
      "Epoch 3, Train loss 1.5980450212955475, Test loss 0.30859414488077164\n",
      "Epoch 4, Train loss 1.2448749160394073, Test loss 0.23838997539132833\n"
     ]
    }
   ],
   "source": [
    "autoencoders = {}\n",
    "\n",
    "for i in range(NUM_TASK):\n",
    "    if(i>2):\n",
    "        break\n",
    "    autoencoders[i] = train_autoencoder(permute_train_loaders[i], permute_test_loaders[i], Autoencoder(input_dims=28*28, code_dims=250))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "66d2d98f-d7d9-4f09-882b-dbfef3b41c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#losses = [0]*NUM_TASK\n",
    "#for i in range(NUM_TASK):\n",
    "#    visulize_test_autoencoder(permute_test_loaders[i], autoencoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1d69220-5d23-44cc-929a-b3a896548a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for _, model in autoencoders.items():\n",
    "#    print(model.mean - 3 * model.std, model.mean + 3 * model.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "34f91dff-9550-4d35-b0e1-9da48ce067d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0343], device='cuda:0') tensor([0.0491], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = autoencoders[0]\n",
    "print(model.mean - 3 * model.std, model.mean + 3 * model.std)\n",
    "images, labels, _ = next(iter(permute_train_loaders[0]))\n",
    "model_outputs = model.get_prediction(images)\n",
    "unreduced_loss = model.get_unreduced_loss(model_outputs, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2323f2e6-ccd8-4b5e-9cf0-a83e3c292802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_mean = torch.mean(unreduced_loss, axis=1)\n",
    "element_mean\n",
    "mean = torch.mean(element_mean)\n",
    "mean\n",
    "x = [item>model.mean - 3 * model.std and item<model.mean + 3 * model.std for item in element_mean]\n",
    "sumX = sum(x)\n",
    "sumX.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b1c7039-2ef0-4bd0-82fa-54fcb67e7d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1000\n"
     ]
    }
   ],
   "source": [
    "index=1\n",
    "\n",
    "losses = [0]*len(autoencoders)\n",
    "passes=0\n",
    "errors=0\n",
    "for _, data in enumerate(permute_train_loaders[index]):\n",
    "    images, labels, _ = data\n",
    "    for i in range(len(autoencoders)):\n",
    "        model = autoencoders[i]\n",
    "        model_outputs = model.get_prediction(images)\n",
    "        losses[i] = model.get_reduced_loss(model_outputs, images)\n",
    "    if(np.argmin(losses)==index):\n",
    "        passes+=1\n",
    "    else:\n",
    "        errors+=1\n",
    "        print(losses)\n",
    "print(f\"{errors}, {passes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c40eda4f-8a3a-4db9-bbe4-4ebeb70f6a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_relatedness(reconstruction_error_autoencoder, reconstruction_error_data):\n",
    "    with torch.no_grad():\n",
    "        return torch.abs((reconstruction_error_autoencoder - reconstruction_error_data)/reconstruction_error_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "930846a3-d538-4c93-83b4-d50863c80dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_autoencoders(batchwise_data, autoencoders):\n",
    "    with torch.no_grad():\n",
    "        minimum_relatedness = float(\"inf\")\n",
    "        best_index=-1\n",
    "        for index, autoencoder in autoencoders.items():\n",
    "            prediction = autoencoder.get_prediction(batchwise_data)\n",
    "            reconstruction_error_data = autoencoder.get_reduced_loss(prediction, batchwise_data)\n",
    "            reconstruction_error_autoencoder = autoencoder.mean\n",
    "            relatedness = find_relatedness(reconstruction_error_autoencoder, reconstruction_error_data)\n",
    "            if relatedness.item()<minimum_relatedness:\n",
    "                minimum_relatedness=relatedness\n",
    "                best_index = index\n",
    "        return best_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea62b19-edaa-4afa-8373-977879828760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_conbined_permute_mnist(num_task, batch_size, random_seed):\n",
    "    datasets = {}\n",
    "    np.random.seed(random_seed)\n",
    "    idx = list(range(28 * 28))\n",
    "    for i in range(num_task):\n",
    "        datasets[i] = PermutedMNIST(train=True, permute_idx=idx, task_num=i)\n",
    "        np.random.shuffle(idx)\n",
    "    dataset = torch.utils.data.ConcatDataset([x for _,x in datasets.items()])\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle = False,\n",
    "                                                  num_workers = 4)\n",
    "    \n",
    "    return random.sample(list(dataloader), len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82aaf110-a376-4506-b83d-f7616c7be497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_num_of_outliers(batchwise_data, model):\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model.get_prediction(batchwise_data)\n",
    "        unreduced_loss = model.get_unreduced_loss(model_outputs, batchwise_data)\n",
    "        element_mean = torch.mean(unreduced_loss, axis=1)\n",
    "        outliers = batchwise_data.shape[0] - sum([item>model.mean - 3 * model.std and item<model.mean + 3 * model.std for item in element_mean]).item()\n",
    "        return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70799744-7723-405d-af99-d5e5503c51c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auto_list = {}\n",
    "expert_list = {}\n",
    "\n",
    "outlier_threshold = 0.3*BATCH_SIZE\n",
    "\n",
    "for _, data in enumerate(permute_train_loaders[0]):\n",
    "    images, labels, index = data\n",
    "    \n",
    "    if len(auto_list)==0:\n",
    "        best_autoencoder = Autoencoder(input_dims=28*28, code_dims=300)\n",
    "        for epoch in range(500):\n",
    "            best_autoencoder.optimize_params(images, images)\n",
    "        auto_list[len(auto_list)] = best_autoencoder\n",
    "        #to-do add initial expert\n",
    "        continue\n",
    "        \n",
    "    best_index = find_best_autoencoders(images, auto_list)\n",
    "    best_autoencoder = auto_list[best_index]\n",
    "    \n",
    "    outliers = find_num_of_outliers(images, best_autoencoder)\n",
    "    \n",
    "    if outliers > outlier_threshold:\n",
    "        best_autoencoder = Autoencoder(input_dims=28*28, code_dims=300)\n",
    "        for epoch in range(5):\n",
    "            best_autoencoder.optimize_params(images, images)\n",
    "        auto_list[len(auto_list)] = best_autoencoder\n",
    "        #to-do add new expert\n",
    "    else:\n",
    "        best_autoencoder.optimize_params(images, images)\n",
    "        #to-do train exsisting expert\n",
    "        \n",
    "    sampleImage, _, _ = next(iter(permute_train_loaders[2]))\n",
    "    sampleOutliers = find_num_of_outliers(sampleImage, best_autoencoder)\n",
    "    print(outliers, sampleOutliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ccc2b217-9459-4d2e-99f3-c8477f6902d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b5e39030-cd78-4235-baff-24889c6bcf46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = get_conbined_permute_mnist(NUM_TASK, BATCH_SIZE, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3ca3edde-3a2f-4bf2-8d7b-aa18726266a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels, _ = next(iter(train_loader))\n",
    "image = images[0].cpu().detach().numpy().reshape((28,28))\n",
    "plt.imshow(image) # Plot the 28x28 image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f1570836-4a75-4c3e-b434-60b410f368f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#5 tasks - 400 training epoches\n",
    "NEW_AUTOENCODER_EPOCH = 700\n",
    "TRAIN_AUTOENCODER_EPOCH = 10\n",
    "CODE_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "27e61436-4906-4b00-a353-57b8947faa5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[new @ batch 0] autoencoder at 0 for index: 3\n",
      "[new @ batch 1] autoencoder at 1 for index: 15\n",
      "[new @ batch 2] autoencoder at 2 for index: 18\n",
      "[new @ batch 3] autoencoder at 3 for index: 17\n",
      "[new @ batch 4] autoencoder at 4 for index: 2\n",
      "[new @ batch 5] autoencoder at 5 for index: 13\n",
      "[new @ batch 6] autoencoder at 6 for index: 5\n",
      "[new @ batch 8] autoencoder at 7 for index: 11\n",
      "[new @ batch 10] autoencoder at 8 for index: 16\n",
      "[new @ batch 11] autoencoder at 9 for index: 12\n",
      "[new @ batch 14] autoencoder at 10 for index: 0\n",
      "[new @ batch 15] autoencoder at 11 for index: 14\n",
      "[new @ batch 19] autoencoder at 12 for index: 19\n",
      "[new @ batch 22] autoencoder at 13 for index: 9\n",
      "[new @ batch 34] autoencoder at 14 for index: 10\n",
      "[new @ batch 40] autoencoder at 15 for index: 4\n",
      "[new @ batch 42] autoencoder at 16 for index: 6\n",
      "[new @ batch 44] autoencoder at 17 for index: 7\n",
      "[new @ batch 45] autoencoder at 18 for index: 8\n",
      "[new @ batch 84] autoencoder at 19 for index: 1\n"
     ]
    }
   ],
   "source": [
    "auto_list = {}\n",
    "expert_list = {}\n",
    "\n",
    "outlier_threshold = 0.003*BATCH_SIZE\n",
    "\n",
    "#debug\n",
    "record = {}\n",
    "\n",
    "for i, data in enumerate(train_loader):\n",
    "    images, labels, indicies = data\n",
    "    \n",
    "    #initial \n",
    "    if len(auto_list)==0:\n",
    "        #debug\n",
    "        print(f\"[new @ batch {i}] autoencoder at {len(auto_list)} for index: {indicies[0].item()}\")\n",
    "        record[indicies[0].item()] = len(auto_list)\n",
    "        \n",
    "        #initial autoencoder\n",
    "        new_autoencoder = Autoencoder(input_dims=28*28, code_dims=CODE_DIM)\n",
    "        for epoch in range(NEW_AUTOENCODER_EPOCH):\n",
    "            new_autoencoder.optimize_params(images, images)\n",
    "        auto_list[len(auto_list)] = new_autoencoder\n",
    "        \n",
    "        #to-do add initial expert\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    #find best autoencoder\n",
    "    best_index = find_best_autoencoders(images, auto_list)\n",
    "    best_autoencoder = auto_list[best_index]\n",
    "    \n",
    "    #calculate outliers\n",
    "    outliers = find_num_of_outliers(images, best_autoencoder)\n",
    "    #print(f\"outliers for best autoencoders {best_index}: {outliers}\")\n",
    "    \n",
    "    if outliers > outlier_threshold:\n",
    "        #debug\n",
    "        print(f\"[new @ batch {i}] autoencoder at {len(auto_list)} for index: {indicies[0].item()}\")\n",
    "        if indicies[0].item() in record:\n",
    "            print(\"error\")\n",
    "        else:\n",
    "            record[indicies[0].item()] = len(auto_list)\n",
    "        \n",
    "        #add new autoencoder\n",
    "        best_autoencoder = Autoencoder(input_dims=28*28, code_dims=CODE_DIM)\n",
    "        for epoch in range(NEW_AUTOENCODER_EPOCH):\n",
    "            best_autoencoder.optimize_params(images, images)\n",
    "        auto_list[len(auto_list)] = best_autoencoder\n",
    "        \n",
    "        #to-do add new expert\n",
    "        \n",
    "    else:\n",
    "        #debug\n",
    "        #print(f\"training autoencoder at {best_index} with index: {indicies[0].item()}\")\n",
    "        if not indicies[0].item() in record or record[indicies[0].item()] != best_index:\n",
    "            print(f\"[error @ batch {i}] training autoencoder at {best_index} with index: {indicies[0].item()}\")\n",
    "        \n",
    "        #train best autoencoder\n",
    "        for epoch in range(TRAIN_AUTOENCODER_EPOCH):\n",
    "            best_autoencoder.optimize_params(images, images)\n",
    "        \n",
    "        #to-do train exsisting expert\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa0f79-863e-4dcc-a281-aafecef8920e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
